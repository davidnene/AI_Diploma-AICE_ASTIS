{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customer_Analytics_Deep_Learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Customer Analytics using Deep Learning"
      ],
      "metadata": {
        "id": "HXp9CcH-pfXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Background information**\n",
        "An Audio book company wants to be able to predict whether customers given 2 years trial period will convert in the next 6 months after trial period.\n",
        "\n",
        "* **Metrics of Success**\n",
        "Developing a fully functioning high accuracy deep learning model that can be able to predict whether a subscriber will convert or not."
      ],
      "metadata": {
        "id": "WBnMhoDFpqNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dVPBoZzwendh"
      },
      "outputs": [],
      "source": [
        "#Importing the required prerequisites\n",
        "\n",
        "#Import pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "#Import pandas for scientific computation\n",
        "import numpy as np\n",
        "\n",
        "#Import StandardScaler to standardize the values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Import pickle to save models\n",
        "import pickle\n",
        "\n",
        "#Import tensorflow\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model preprocessing"
      ],
      "metadata": {
        "id": "4bNbRjv2rqOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extract data from CSV"
      ],
      "metadata": {
        "id": "Akt_VIb-tD6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data in an array form\n",
        "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
        "\n",
        "#extract inputs exluding first and last columns\n",
        "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
        "\n",
        "#Extract targets\n",
        "targets_all = raw_csv_data[:, -1]"
      ],
      "metadata": {
        "id": "pNZXCtuZiTxM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Balance the dataset"
      ],
      "metadata": {
        "id": "V2F6UTgEtNC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_one_targets = int(np.sum(targets_all))\n",
        "\n",
        "zero_targets_counter = 0\n",
        "indices_to_remove = []\n",
        "\n",
        "#create a for loop\n",
        "for i in range(targets_all.shape[0]):\n",
        "  if targets_all[i] == 0:\n",
        "    zero_targets_counter += 1\n",
        "    if zero_targets_counter > num_one_targets:\n",
        "      indices_to_remove.append(i)\n",
        "\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
      ],
      "metadata": {
        "id": "k2ZVSK7KtACE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Standardize inputs\n",
        "This will help the model perform better"
      ],
      "metadata": {
        "id": "h3z-BUFWvhdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Give the scaler an identifier\n",
        "scaler_deep_learning = StandardScaler()\n",
        "\n",
        "#Fit the scaler to the balanced input data\n",
        "scaled_inputs = scaler_deep_learning.fit_transform(unscaled_inputs_equal_priors)"
      ],
      "metadata": {
        "id": "CVu2yMv4vdSx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shuffle the data"
      ],
      "metadata": {
        "id": "JtuoWleFwS2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data might be ordered by date, hence become homogeneous at certain points\n",
        "#We'll need to shaffle the data to make it as randomized as possible so that it\n",
        "#won't affect the gradient descent\n",
        "\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ],
      "metadata": {
        "id": "vFA4q3oEwZia"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split the dataset into train, validation and test"
      ],
      "metadata": {
        "id": "-Ootny7Nxohn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8*samples_count)\n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "#split datasets into inputs and targets\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "#Checking whether the target values are half or 50% of the total\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Xy5x6zxVQ_",
        "outputId": "1cd625a6-b300-476b-e6ca-e5416b546283"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1821.0 3579 0.5088013411567477\n",
            "218.0 447 0.48769574944071586\n",
            "198.0 448 0.4419642857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proportions of classes are almost equal at around 50%"
      ],
      "metadata": {
        "id": "ZykyZmfB2cF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving the three datasets in *.npz\n",
        "This is to ensure they are conversant with tensorflow"
      ],
      "metadata": {
        "id": "FabP1ivT3Cih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
      ],
      "metadata": {
        "id": "6PMjGHVB3O6G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving the Stardadization Scaler"
      ],
      "metadata": {
        "id": "WDzQ6a_v4LDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(scaler_deep_learning, open('scaler_deep_learning.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "JeXJMzwG4Kht"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Outlining"
      ],
      "metadata": {
        "id": "BzrubE1C7p0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading data"
      ],
      "metadata": {
        "id": "NX8uR_PJ8Xim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train data\n",
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs = npz['inputs'].astype(np.float64)\n",
        "train_targets = npz['targets'].astype(np.int64)\n",
        "\n",
        "#validation data\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(np.float64), npz['targets'].astype(np.int64)\n",
        "\n",
        "#test data\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "test_inputs, test_targets = npz['inputs'].astype(np.float64), npz['targets'].astype(np.int64)"
      ],
      "metadata": {
        "id": "EewJqaUa8bKk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up layers\n"
      ],
      "metadata": {
        "id": "nVHxH1ROAhV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_size = 10 (predictors)\n",
        "output_size = 2\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
        "                             ])"
      ],
      "metadata": {
        "id": "zB46FWynAk0y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model compiling"
      ],
      "metadata": {
        "id": "ycpMdbEHBXZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "max_epochs = 100\n",
        "\n",
        "#Early stopping controls overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "#Fitting the model\n",
        "model.fit(\n",
        "    train_inputs,\n",
        "    train_targets,\n",
        "    batch_size = batch_size,\n",
        "    epochs = max_epochs,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data = (validation_inputs, validation_targets),\n",
        "    verbose = 2\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR-zxqEpBdpx",
        "outputId": "0ac3f851-2e7e-4bb7-fef2-f1b4954faa3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 6.1932 - accuracy: 0.1676 - val_loss: 3.3455 - val_accuracy: 0.3915 - 624ms/epoch - 17ms/step\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 1.7032 - accuracy: 0.4395 - val_loss: 0.8683 - val_accuracy: 0.4631 - 98ms/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.7542 - accuracy: 0.5155 - val_loss: 0.6796 - val_accuracy: 0.5436 - 112ms/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.6402 - accuracy: 0.6024 - val_loss: 0.5997 - val_accuracy: 0.6331 - 103ms/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.5811 - accuracy: 0.6778 - val_loss: 0.5392 - val_accuracy: 0.7025 - 107ms/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.5386 - accuracy: 0.7404 - val_loss: 0.4974 - val_accuracy: 0.7763 - 102ms/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.5030 - accuracy: 0.7974 - val_loss: 0.4580 - val_accuracy: 0.8054 - 105ms/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.4682 - accuracy: 0.8248 - val_loss: 0.4270 - val_accuracy: 0.8188 - 98ms/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.4440 - accuracy: 0.8452 - val_loss: 0.4119 - val_accuracy: 0.8456 - 113ms/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.4155 - accuracy: 0.8488 - val_loss: 0.3878 - val_accuracy: 0.8591 - 95ms/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.3963 - accuracy: 0.8553 - val_loss: 0.3750 - val_accuracy: 0.8635 - 128ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.3768 - accuracy: 0.8625 - val_loss: 0.3560 - val_accuracy: 0.8658 - 98ms/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.3594 - accuracy: 0.8684 - val_loss: 0.3371 - val_accuracy: 0.8658 - 98ms/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "36/36 - 0s - loss: 0.3553 - accuracy: 0.8726 - val_loss: 0.3303 - val_accuracy: 0.8792 - 118ms/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "36/36 - 0s - loss: 0.3436 - accuracy: 0.8785 - val_loss: 0.3038 - val_accuracy: 0.8859 - 90ms/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "36/36 - 0s - loss: 0.3426 - accuracy: 0.8779 - val_loss: 0.2965 - val_accuracy: 0.8837 - 106ms/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "36/36 - 0s - loss: 0.3373 - accuracy: 0.8765 - val_loss: 0.2780 - val_accuracy: 0.8971 - 108ms/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "36/36 - 0s - loss: 0.3239 - accuracy: 0.8804 - val_loss: 0.2834 - val_accuracy: 0.8926 - 91ms/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "36/36 - 0s - loss: 0.3173 - accuracy: 0.8829 - val_loss: 0.2717 - val_accuracy: 0.8971 - 97ms/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "36/36 - 0s - loss: 0.3105 - accuracy: 0.8868 - val_loss: 0.2666 - val_accuracy: 0.8949 - 103ms/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "36/36 - 0s - loss: 0.3108 - accuracy: 0.8880 - val_loss: 0.2844 - val_accuracy: 0.8993 - 102ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "36/36 - 0s - loss: 0.2996 - accuracy: 0.8880 - val_loss: 0.2717 - val_accuracy: 0.8926 - 93ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f217592b850>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has an accuracy of 89% on validation set"
      ],
      "metadata": {
        "id": "RwOke3M3E0mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model"
      ],
      "metadata": {
        "id": "0Zbl_ae4FFl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs,test_targets)\n",
        "print('\\nTest loss: {0:.2f}. Test accurac  y: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmA74jfNFplf",
        "outputId": "1ec18776-e836-422b-a441-fcc70bc85175"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.9040\n",
            "\n",
            "Test loss: 0.31. Test accurac  y: 90.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performed much better on the test set with a 90% accuracy"
      ],
      "metadata": {
        "id": "h6bKQAnSFEz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtaining the probability of classes\n",
        "\n",
        "np.argmax(model.predict(test_inputs), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgEhcL6eF73j",
        "outputId": "1a205c67-0010-4820-a775-40e817bf545c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving the model"
      ],
      "metadata": {
        "id": "q6JLiRz1Gc5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('audiobooks_model.h5')"
      ],
      "metadata": {
        "id": "Ws-drneaGmhh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicting new data\n",
        "\n",
        "Let's assume we want to use the model to predict new instances."
      ],
      "metadata": {
        "id": "e8XOAYVrHY4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import prerequisites"
      ],
      "metadata": {
        "id": "cEie5s9lHeGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle"
      ],
      "metadata": {
        "id": "v2snUzyVHw4F"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the scaler and the model"
      ],
      "metadata": {
        "id": "yMK3X1Z9IIO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_deep_learning = pickle.load(open('scaler_deep_learning.pickle','rb'))\n",
        "model = tf.keras.models.load_model('audiobooks_model.h5')"
      ],
      "metadata": {
        "id": "Dcnlwi57IN9V"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load new data"
      ],
      "metadata": {
        "id": "5mERobe_Izbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = np.loadtxt('New_Audiobooks_Data.csv', delimiter = ',')\n",
        "new_data_inputs = raw_data[:,1:]"
      ],
      "metadata": {
        "id": "7yySG41OJQ6Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict the probability of  customer to convert"
      ],
      "metadata": {
        "id": "a9OpFRhjJr6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_inputs_scaled = scaler_deep_learning.transform(new_data_inputs)"
      ],
      "metadata": {
        "id": "DEtE6AvNJ47_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(model.predict(new_data_inputs_scaled), 1)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNhBTcEhKJ8e",
        "outputId": "2052bf48-2478-4f5c-960e-9c64998b38b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}